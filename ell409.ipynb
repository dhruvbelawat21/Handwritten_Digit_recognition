{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11376625,"sourceType":"datasetVersion","datasetId":7122355,"isSourceIdPinned":true},{"sourceId":11376632,"sourceType":"datasetVersion","datasetId":7122364,"isSourceIdPinned":true},{"sourceId":11728016,"sourceType":"datasetVersion","datasetId":7122146}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"Creating the Environment","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copy setup files from your Kaggle dataset to the working directory\n!cp /kaggle/input/setupfiles/environment.yml /kaggle/working/\n!cp /kaggle/input/setupfiles/install.sh /kaggle/working/\n\n# Run the installation script\n!bash install.sh","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:33:26.805815Z","iopub.execute_input":"2025-05-08T06:33:26.806192Z","iopub.status.idle":"2025-05-08T06:33:29.944471Z","shell.execute_reply.started":"2025-05-08T06:33:26.806165Z","shell.execute_reply":"2025-05-08T06:33:29.943747Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies from environment.yml using pip...\nEnvironment setup completed successfully!\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"Loading the Datasets on Kaggle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/setupfiles/vae.py /kaggle/working/\n!cp -r /kaggle/input/trainingdataset /kaggle/working/\n!cp -r /kaggle/input/recon-dataset /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Running the Code ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!export CUDA_LAUNCH_BLOCKING=1\n!python vae.py /kaggle/input/trainingdataset/mnist_1_4_8_train.npz /kaggle/input/recon-dataset/mnist_1_4_8_val_recon.npz train vae.pth gmm_params.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:34:43.336486Z","iopub.execute_input":"2025-05-08T06:34:43.336763Z","iopub.status.idle":"2025-05-08T06:36:35.242211Z","shell.execute_reply.started":"2025-05-08T06:34:43.336727Z","shell.execute_reply":"2025-05-08T06:36:35.241509Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 186.28585129961115, Validation Loss: 132.69738221964587, Learning Rate: 0.001\nEpoch 2, Training Loss: 123.76245744197242, Validation Loss: 121.62170801275295, Learning Rate: 0.001\nEpoch 3, Training Loss: 117.97477372190997, Validation Loss: 118.61028124341273, Learning Rate: 0.001\nEpoch 4, Training Loss: 115.51460922352437, Validation Loss: 115.20525565187606, Learning Rate: 0.001\nEpoch 5, Training Loss: 114.38577349660032, Validation Loss: 114.50161108110244, Learning Rate: 0.001\nEpoch 6, Training Loss: 113.0163465785382, Validation Loss: 113.10102168528668, Learning Rate: 0.001\nEpoch 7, Training Loss: 112.11463650155217, Validation Loss: 113.87202683653035, Learning Rate: 0.001\nEpoch 8, Training Loss: 111.66392089512655, Validation Loss: 112.60997394735455, Learning Rate: 0.001\nEpoch 9, Training Loss: 111.23342674605865, Validation Loss: 111.98047014979447, Learning Rate: 0.001\nEpoch 10, Training Loss: 110.80757729064248, Validation Loss: 111.13859579205312, Learning Rate: 0.001\nEpoch 11, Training Loss: 110.38282212242673, Validation Loss: 111.6873019702519, Learning Rate: 0.001\nEpoch 12, Training Loss: 110.22372715366879, Validation Loss: 110.56395842116358, Learning Rate: 0.001\nEpoch 13, Training Loss: 109.85508412194466, Validation Loss: 110.38613470633959, Learning Rate: 0.001\nEpoch 14, Training Loss: 109.63261336750006, Validation Loss: 110.8555665709317, Learning Rate: 0.001\nEpoch 15, Training Loss: 109.29446262657733, Validation Loss: 110.2197236805702, Learning Rate: 0.001\nEpoch 16, Training Loss: 108.97391631195435, Validation Loss: 110.73686383458052, Learning Rate: 0.001\nEpoch 17, Training Loss: 108.94557584409169, Validation Loss: 109.82251182414629, Learning Rate: 0.001\nEpoch 18, Training Loss: 108.66892677122182, Validation Loss: 109.74668084027192, Learning Rate: 0.001\nEpoch 19, Training Loss: 108.6051815050419, Validation Loss: 110.39036307375106, Learning Rate: 0.001\nEpoch 20, Training Loss: 108.52396158081613, Validation Loss: 109.74237968354764, Learning Rate: 0.001\nEpoch 21, Training Loss: 108.30068928857115, Validation Loss: 110.98354607464692, Learning Rate: 0.001\nEpoch 22, Training Loss: 107.96652895766894, Validation Loss: 109.82609134564713, Learning Rate: 0.001\nEpoch 23, Training Loss: 108.11124895208668, Validation Loss: 109.7388615883221, Learning Rate: 0.001\nEpoch 24, Training Loss: 107.82876630887517, Validation Loss: 109.77406658410624, Learning Rate: 0.0005\nEpoch 25, Training Loss: 106.60155952015172, Validation Loss: 108.5538192980607, Learning Rate: 0.0005\nEpoch 26, Training Loss: 106.19469022904438, Validation Loss: 108.73845153088111, Learning Rate: 0.0005\nEpoch 27, Training Loss: 106.10993752913629, Validation Loss: 108.4476821050274, Learning Rate: 0.0005\nEpoch 28, Training Loss: 106.03056395201385, Validation Loss: 108.56140651349072, Learning Rate: 0.0005\nEpoch 29, Training Loss: 105.90370714440147, Validation Loss: 108.96217088361615, Learning Rate: 0.0005\nEpoch 30, Training Loss: 105.74283029879932, Validation Loss: 108.72976284187922, Learning Rate: 0.0005\nEpoch 31, Training Loss: 105.74969800065425, Validation Loss: 108.64024829060392, Learning Rate: 0.0005\nEpoch 32, Training Loss: 105.60300118730399, Validation Loss: 108.58274596859191, Learning Rate: 0.0005\nEpoch 33, Training Loss: 105.44999340708569, Validation Loss: 108.66163212742411, Learning Rate: 0.00025\nEpoch 34, Training Loss: 104.67757184910843, Validation Loss: 108.15625790472176, Learning Rate: 0.00025\nEpoch 35, Training Loss: 104.47305031424156, Validation Loss: 108.31759763978182, Learning Rate: 0.00025\nEpoch 36, Training Loss: 104.36806578958695, Validation Loss: 108.35957747615409, Learning Rate: 0.00025\nEpoch 37, Training Loss: 104.2656065001086, Validation Loss: 108.20389241673693, Learning Rate: 0.00025\nEpoch 38, Training Loss: 104.19529139274135, Validation Loss: 108.49363859282778, Learning Rate: 0.00025\nEpoch 39, Training Loss: 104.11836476769766, Validation Loss: 108.36495919187395, Learning Rate: 0.00025\nEpoch 40, Training Loss: 104.01673971157717, Validation Loss: 108.42972175379427, Learning Rate: 0.000125\nEpoch 41, Training Loss: 103.55398458690033, Validation Loss: 108.15802032830945, Learning Rate: 0.000125\nEpoch 42, Training Loss: 103.4330066474621, Validation Loss: 108.22063437038891, Learning Rate: 0.000125\nEpoch 43, Training Loss: 103.37858980666745, Validation Loss: 108.38876417909465, Learning Rate: 0.000125\nEpoch 44, Training Loss: 103.32200623334596, Validation Loss: 108.3445464007167, Learning Rate: 0.000125\nEpoch 45, Training Loss: 103.2916638661611, Validation Loss: 108.40358347386172, Learning Rate: 0.000125\nEpoch 46, Training Loss: 103.21166280334856, Validation Loss: 108.42799366634169, Learning Rate: 6.25e-05\nEpoch 47, Training Loss: 102.95554838230659, Validation Loss: 108.3338894359981, Learning Rate: 6.25e-05\nEpoch 48, Training Loss: 102.87718422878878, Validation Loss: 108.31083465627636, Learning Rate: 6.25e-05\nEarly stopping triggered after 49 epochs.\nGMM converged after 59 iterations\nGMM converged\nFigure(1200x1000)\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"\n!python vae.py /kaggle/input/recon-dataset/mnist_1_4_8_val_recon.npz test_reconstruction vae.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:39:30.806366Z","iopub.execute_input":"2025-05-08T06:39:30.807135Z","iopub.status.idle":"2025-05-08T06:39:36.821522Z","shell.execute_reply.started":"2025-05-08T06:39:30.807104Z","shell.execute_reply":"2025-05-08T06:39:36.820647Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/vae.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\nNumber of trainable parameters in MLPVAE: 7533317\nFigure(1000x1000)\nImage 1 SSIM score: 0.9087\nImage 2 SSIM score: 0.9151\nImage 3 SSIM score: 0.9409\nImage 4 SSIM score: 0.8985\nImage 5 SSIM score: 0.9182\nImage 6 SSIM score: 0.5703\nImage 7 SSIM score: 0.5734\nImage 8 SSIM score: 0.4946\nImage 9 SSIM score: 0.5945\nImage 10 SSIM score: 0.5616\nImage 11 SSIM score: 0.2973\nImage 12 SSIM score: 0.6589\nImage 13 SSIM score: 0.6306\nImage 14 SSIM score: 0.7221\nImage 15 SSIM score: 0.6565\nAverage : 0.6894\nReconstruction image saved at reconstructed_total.png\nReconstructed images saved to 'vae_reconstructed.npz')\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"\n!python vae.py /kaggle/input/recon-dataset/mnist_1_4_8_val_recon.npz test_classifier vae.pth gmm_params.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:39:39.479293Z","iopub.execute_input":"2025-05-08T06:39:39.479575Z","iopub.status.idle":"2025-05-08T06:39:44.248870Z","shell.execute_reply.started":"2025-05-08T06:39:39.479550Z","shell.execute_reply":"2025-05-08T06:39:44.248178Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/vae.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\nGMM Performance Metrics: {'accuracy': 1.0, 'precision_macro': 1.0, 'recall_macro': 1.0, 'f1_macro': 1.0}\n","output_type":"stream"}],"execution_count":56}]}